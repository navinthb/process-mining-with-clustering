{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic analysis of the nature of the event log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Raw Event data in XES format as an Event Log ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pm4py\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "\n",
    "hospital_log_all = xes_importer.apply('Data\\\\Hospital_log_all.xes')\n",
    "\n",
    "print('Number of traces present in the full event log:', len(hospital_log_all))\n",
    "\n",
    "num_of_events = 0\n",
    "for trace in hospital_log_all:\n",
    "    num_of_events = num_of_events + len(trace)\n",
    "\n",
    "print('Number of events in full event log:', num_of_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility code to automatically load newly compiled classes into Jupiter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "ip = get_ipython()\n",
    "ip.magic(\"reload_ext autoreload\")\n",
    "ip.magic(\"autoreload 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "# Configure autoreload to automatically reload all modules\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting a subset of the full event log based on traces ##\n",
    "\n",
    "We consider 33.3% of the traces in the full event log. This is for the ease of computation later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Preprocessing import DivideDatasets\n",
    "\n",
    "hospital_log_initial = DivideDatasets.get_subset(hospital_log_all, 3)\n",
    "\n",
    "print('Number of traces present in the event log subset:', len(hospital_log_initial))\n",
    "\n",
    "num_of_events = 0\n",
    "for trace in hospital_log_initial:\n",
    "     num_of_events = num_of_events + len(trace)\n",
    "\n",
    "print('Number of events in the event log subset:', num_of_events)\n",
    "\n",
    "pm4py.write_xes(hospital_log_initial, 'Data\\Processed\\Hospital_Log_Initial.xes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical & statistical analysis of the selected dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "\n",
    "hospital_log_analysis_df = log_converter.apply(hospital_log_initial, variant=log_converter.Variants.TO_DATA_FRAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_log_analysis_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_log_analysis_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of events per trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get the count group by 'case:concept:name'\n",
    "event_count_per_trace = hospital_log_analysis_df.groupby('case:concept:name').size()\n",
    "\n",
    "event_count_per_trace.describe().round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The oldest & the newest trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Oldest trace: ',hospital_log_analysis_df['time:timestamp'].min())\n",
    "print('Newest trace: ',hospital_log_analysis_df['time:timestamp'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age range of the patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_log_analysis_df['case:Age'].describe().round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most widely used event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get the count for events names\n",
    "event_counts = hospital_log_analysis_df['concept:name'].value_counts()\n",
    "\n",
    "print(\"Most widely recorded event:\", event_counts.idxmax())\n",
    "print(\"Number of occurrences:\", event_counts.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The org group which the most number of patients visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Group pandas Data Frame by the 'case:concept_name' \n",
    "# and get the first instance of 'org:group'\n",
    "org_group_usage= hospital_log_analysis_df.groupby('case:concept:name')['org:group'].first()\n",
    "\n",
    "# Count the occurrences of each 'org:group'\n",
    "org_group_counts = org_group_usage.value_counts()\n",
    "\n",
    "# Get the 'org:group' inthe most of traces\n",
    "most_recorded_org_group = org_group_counts.idxmax()\n",
    "max_org_group_count = org_group_counts.max()\n",
    "\n",
    "print(\"Org group recorded in most of the traces:\", most_recorded_org_group)\n",
    "print(\"Number of instances:\", max_org_group_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``When starting from middle with divided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pm4py\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.objects.conversion.log import converter as xes_converter\n",
    "\n",
    "hospital_log_initial = xes_importer.apply('Data\\Processed\\Hospital_Log_Initial.xes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the public Event Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating the event log content into English\n",
    "\n",
    "Mainly the activity name is in English. For a better clarity while analysing, we convert that to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hospital_log_initial_temp = hospital_log_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Preprocessing import TranslationManager\n",
    "\n",
    "hospital_log_translated = TranslationManager.translate(hospital_log_initial, 'en')\n",
    "\n",
    "pm4py.write_xes(hospital_log_translated, \"Data\\Processed\\Hospital_Log_Translated.xes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``When starting from middle (since Translation is costly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pm4py\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.objects.conversion.log import converter as xes_converter\n",
    "\n",
    "hospital_log_translated = xes_importer.apply('Data\\Processed\\Hospital_Log_Translated.xes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hospital_log_translated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove duplicates\n",
    "\n",
    "The logic here we are using is:\n",
    "  1. Extract traces & events into two lists from the original EventLog\n",
    "  2. Create a Pandas dataframe using above two lists as columns & remove duplicates from there\n",
    "  3. Rebuild the EventLog referring to original EventLog by only adding values persent in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pm4py.utils as pm4py_utils\n",
    "\n",
    "# Define neww lists for trace and event data\n",
    "all_traces = []\n",
    "all_events = []\n",
    "\n",
    "# # Get trace and event data from the event log object\n",
    "\n",
    "# ###\n",
    "# ###\n",
    "# #translation seems giving low hits\n",
    "# #for trace in hospital_log_translated:\n",
    "# ###\n",
    "# ###\n",
    "\n",
    "for trace in hospital_log_initial:\n",
    "    # Get trace name for each trace\n",
    "    trace_name = trace.attributes['concept:name']\n",
    "\n",
    "    # Make the event name as unique as possible\n",
    "    for event in trace:\n",
    "        all_traces.append(trace_name)\n",
    "        # Extract event name\n",
    "        event_name = event['concept:name'] + ' - ' + event['Producer code'] + str(event['Specialism code']) + ' - ' + str(event['time:timestamp'])\n",
    "        if 'org:group' in event:\n",
    "            event_name = event_name + ' - ' + event['org:group'] \n",
    "        if 'Section' in event:\n",
    "            event_name = event_name + ' - ' + event['Section']\n",
    "            \n",
    "        all_events.append(event_name)\n",
    "\n",
    "# Raise error if the liists length are different\n",
    "if len(all_traces) != len(all_events):\n",
    "    raise ValueError(\"Lengths of all_traces and all_events do not match.\")\n",
    "\n",
    "# Creat a Pandas DataFrame\n",
    "hospital_log_initial_df = pd.DataFrame({'trace': all_traces, 'event': all_events})\n",
    "#print(\"All Events:\", len(hospital_log_translated_df))\n",
    "# Remove duplicates\n",
    "hospital_log_initial_df = hospital_log_initial_df.drop_duplicates(subset=['trace', 'event'])\n",
    "#print(\"Dup removed removed:\", len(hospital_log_translated_df))\n",
    "\n",
    "hospital_log_dup_removed = pm4py_utils.EventLog()\n",
    "\n",
    "# Initialize a dictionary to keep track of added events for each trace\n",
    "events_included = {}\n",
    "\n",
    "# Iterate over the original event log to reconstruct the dupliacate removed event log\n",
    "for trace in hospital_log_initial:\n",
    "    # Get trace name\n",
    "    trace_name = trace.attributes['concept:name']\n",
    "    \n",
    "    # Check if trace_name already exists in the DataFrame\n",
    "    if trace_name in hospital_log_initial_df['trace'].values:\n",
    "        # Create a new trace object\n",
    "        new_trace = pm4py_utils.Trace()\n",
    "        hospital_log_dup_removed.append(new_trace)\n",
    "        # Copy trace attributes\n",
    "        for key, value in trace.attributes.items():\n",
    "            new_trace.attributes[key] = value\n",
    "        # Initialize a set to keep track of added events for the current trace\n",
    "        events_included[trace_name] = set()\n",
    "        # Associate the trace name with all events within the trace\n",
    "        for event in trace:\n",
    "            event_name = event['concept:name'] + ' - ' + event['Producer code'] + str(event['Specialism code']) + ' - ' + str(event['time:timestamp'])\n",
    "            if 'org:group' in event:\n",
    "                event_name = event_name + ' - ' + event['org:group']\n",
    "            if 'Section' in event:\n",
    "                event_name = event_name + ' - ' + event['Section']\n",
    "            \n",
    "            # Check if the event name is in the cleaned DataFrame and not already added\n",
    "            if event_name in hospital_log_initial_df[hospital_log_initial_df['trace'] == trace_name]['event'].values \\\n",
    "                    and event_name not in events_included[trace_name]:\n",
    "                # Create a new event object\n",
    "                new_event = pm4py_utils.Event()\n",
    "                # Copy event attributes\n",
    "                for key, value in event.items():\n",
    "                    new_event[key] = value\n",
    "                # Add the event to the trace\n",
    "                new_trace.append(new_event)\n",
    "                # Add the event name to the set of added events for the current trace\n",
    "                events_included[trace_name].add(event_name)\n",
    "\n",
    "print(\"Number of traces present in the duplicate removed event log:\", len(hospital_log_dup_removed))\n",
    "\n",
    "num_of_events = 0\n",
    "for trace in hospital_log_dup_removed:\n",
    "     num_of_events = num_of_events + len(trace)\n",
    "\n",
    "print('Number of events in the duplicate removed event:', num_of_events)\n",
    "\n",
    "# write into file\n",
    "pm4py.write_xes(hospital_log_dup_removed, 'Data\\Processed\\Hospital_Log_Dup_Removed.xes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``When starting from middle (dup removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pm4py\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.objects.conversion.log import converter as xes_converter\n",
    "\n",
    "hospital_log_dup_removed = xes_importer.apply('Data\\Processed\\Hospital_Log_Dup_Removed.xes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove traces with no events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def count_traces_without_events(Logg):\n",
    "    \n",
    "    # Count traces without events\n",
    "    count = 0\n",
    "    all_traces=0\n",
    "    for trace in Logg:\n",
    "        # Check if trace has any events\n",
    "        all_traces += 1\n",
    "        present = 0\n",
    "        for event in trace:\n",
    "            present += 1\n",
    "        if present==0:\n",
    "            count += 1\n",
    "\n",
    "    return count, all_traces\n",
    "\n",
    "traces_without_events, all_tracesss = count_traces_without_events(hospital_log_dup_removed)\n",
    "print('Number of traces:', all_tracesss)\n",
    "print('Number of traces without any events:', traces_without_events)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove traces with extreme number of events, as outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Boxplot for number of events present in traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hospital_log_dup_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert the event log to a pandas data frame\n",
    "hospital_log_dup_removed_df = log_converter.apply(hospital_log_dup_removed, variant=log_converter.Variants.TO_DATA_FRAME)\n",
    "\n",
    "# Count the number of events per each trace\n",
    "trace_counts = hospital_log_dup_removed_df.groupby('case:concept:name').size()\n",
    "trace_lengths = trace_counts.reset_index(name='event_count')\n",
    "\n",
    "# Plot a boxplot of trace lengths\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.boxplot(trace_lengths['event_count'])\n",
    "plt.title('Number of events per trace')\n",
    "plt.xlabel('Trace')\n",
    "plt.ylabel('Number of Events')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the upper bound for number of traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Interquartile Range (IQR)\n",
    "Q1 = trace_lengths['event_count'].quantile(0.25)\n",
    "Q3 = trace_lengths['event_count'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Calculate the upper bound\n",
    "upper_bound_val = Q3 + (1.5 * IQR)\n",
    "\n",
    "print('Upper Bound for number of events per trace:', round(upper_bound_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of traces beyond upper bound of number of tarces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitler out the traces beyond the upper bound\n",
    "records_beyond_upper_bound = trace_lengths[trace_lengths['event_count'] > round(upper_bound_val)]\n",
    "\n",
    "# Display the number of traces  that meet the condition\n",
    "print('Number of traces with beyond the upper bound of number of tarces:', records_beyond_upper_bound.shape[0])\n",
    "print ('Percentage of traces with beyond 800 event: ', round(records_beyond_upper_bound.shape[0]/len(hospital_log_dup_removed)*100,2), '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anayzing traces beyond upper bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traces with number of events above 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitler out the traces with number of events beyond 800\n",
    "records_beyond_upper_bound = trace_lengths[trace_lengths['event_count'] > 800]\n",
    "\n",
    "# Count & the percentage\n",
    "print('Number of traces with beyond 800 events:', records_beyond_upper_bound.shape[0])\n",
    "print ('Percentage of traces with beyond 800 events: ', round(records_beyond_upper_bound.shape[0]/len(hospital_log_dup_removed)*100,2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traces with number of events above 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitler out the traces with number of events beyond 600\n",
    "records_beyond_amount = trace_lengths[trace_lengths['event_count'] > 600]\n",
    "\n",
    "# Count & the percentage\n",
    "print('Number of traces with beyond 600 events:', records_beyond_amount.shape[0])\n",
    "print ('Percentage of traces with beyond 600 events: ', round(records_beyond_amount.shape[0]/len(hospital_log_dup_removed)*100,2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traces with number of events above 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitler out the traces with number of events beyond 500\n",
    "records_beyond_amount = trace_lengths[trace_lengths['event_count'] > 500]\n",
    "\n",
    "# Count & the percentage\n",
    "print('Number of traces with beyond 500 events:', records_beyond_amount.shape[0])\n",
    "print ('Percentage of traces with beyond 500 events: ', round(records_beyond_amount.shape[0]/len(hospital_log_dup_removed)*100,2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traces with number of events above 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitler out the traces with number of events beyond 400\n",
    "records_beyond_amount = trace_lengths[trace_lengths['event_count'] > 400]\n",
    "\n",
    "# Count & the percentage\n",
    "print('Number of traces with beyond 400 events: ', records_beyond_amount.shape[0])\n",
    "print ('Percentage of traces with beyond 400 events: ', round(records_beyond_amount.shape[0]/len(hospital_log_dup_removed)*100,2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove traces beyond 500 as outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caculate lower bound for number fo traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Interquartile Range (IQR)\n",
    "Q1 = trace_lengths['event_count'].quantile(0.25)\n",
    "Q3 = trace_lengths['event_count'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Calculate the lower bound\n",
    "lower_bound_val = Q1 - (1.5 * IQR)\n",
    "\n",
    "print('Lower Bound for number of events per trace:', round(lower_bound_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having traces around this value is unrealistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anayzing traces below lower bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traces with number of events below 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitler out the traces with number of events below 2\n",
    "records_below_lower_bound = trace_lengths[trace_lengths['event_count'] < 2]\n",
    "\n",
    "# Count & the percentage\n",
    "print('Number of traces with below 2 events: ', records_below_lower_bound.shape[0])\n",
    "print ('Percentage of traces with below 2 events: ', round(records_below_lower_bound.shape[0]/len(hospital_log_dup_removed)*100,2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traces with number of events below 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitler out the traces with number of events below 3\n",
    "records_below_lower_bound = trace_lengths[trace_lengths['event_count'] < 3 ]\n",
    "\n",
    "# Count & the percentage\n",
    "print('Number of traces with below 2 events: ', records_below_lower_bound.shape[0])\n",
    "print ('Percentage of traces with below 2 events: ', round(records_below_lower_bound.shape[0]/len(hospital_log_dup_removed)*100,2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove traces below 2 as outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove traces beyond number of events 500 & below 2 as outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove the selected traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter traces with event count less than or equal to 500\n",
    "filtered_traces = trace_lengths[(trace_lengths['event_count'] <= 500) & (trace_lengths['event_count'] > 1)]\n",
    "\n",
    "# Create a new panda Data Frame with the filtered traces\n",
    "hospital_log_cleansed_df = hospital_log_dup_removed_df[hospital_log_dup_removed_df['case:concept:name'].isin(filtered_traces['case:concept:name'])]\n",
    "\n",
    "hospital_log_cleansed_df['case:concept:name'].nunique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconstruct the event log after removal of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_log_cleansed = pm4py_utils.EventLog()\n",
    "\n",
    "# Initialize a dictionary to keep track of added events for each trace\n",
    "events_included = {}\n",
    "\n",
    "cleansed_trace_names_set = set(hospital_log_cleansed_df['case:concept:name'])\n",
    "\n",
    "# Iterate over the original event log to reconstruct the dupliacate removed event log\n",
    "for trace in hospital_log_initial:\n",
    "    # Get trace name\n",
    "    trace_name = trace.attributes['concept:name']\n",
    "\n",
    "\n",
    "    # Check if the trace name exists in cleansed_trace_names_set\n",
    "    if trace_name in cleansed_trace_names_set:       \n",
    "        new_trace = pm4py_utils.Trace()\n",
    "        hospital_log_cleansed.append(new_trace)\n",
    "        # Copy trace attributes\n",
    "        for key, value in trace.attributes.items():\n",
    "            new_trace.attributes[key] = value\n",
    "        # Initialize a set to keep track of added events for the current trace\n",
    "        events_included[trace_name] = set()\n",
    "        # Associate the trace name with all events within the trace\n",
    "\n",
    "        for event in trace:\n",
    "            event_name = event['concept:name'] + ' - ' + event['Producer code'] + str(event['Specialism code']) + ' - ' + str(event['time:timestamp'])\n",
    "            if 'org:group' in event:\n",
    "                event_name = event_name + ' - ' + event['org:group']\n",
    "            if 'Section' in event:\n",
    "                event_name = event_name + ' - ' + event['Section']\n",
    "            \n",
    "            new_event = pm4py_utils.Event()\n",
    "            # Copy event attributes\n",
    "            for key, value in event.items():\n",
    "                new_event[key] = value\n",
    "            # Add the event to the trace\n",
    "            new_trace.append(new_event)\n",
    "            # Add the event name to the set of added events for the current trace\n",
    "            events_included[trace_name].add(event_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hospital_log_cleansed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_log_cleansed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm4py.write_xes(hospital_log_cleansed, \"Data\\Processed\\Hospital_Log_Cleansed.xes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``When starting from cleansed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "\n",
    "hospital_log_cleansed = xes_importer.apply('Data\\Processed\\Hospital_Log_Cleansed.xes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_log_cleansed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequent Pattern Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding most frequest patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using FP-Growth algorithm\n",
    "\n",
    "We are using the fp growth algorithm here to find frequent flow variants.\n",
    "In creating transactions to be fed into into FP Growth, we use concept:name as the key of each event\n",
    "Also sending the transaction list in chunks to FP Growth algo to process to reduce the complexity in processing,\n",
    "and later merging the results by removing duplicates, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from FrequentPatternMining import FPGrowthHandler, TransactionManager\n",
    "\n",
    "\n",
    "# GEt the list of transactions based on events in each trace\n",
    "transactions_list = TransactionManager.create_transactions(hospital_log_cleansed)\n",
    "\n",
    "# Split transactions list into chunks \n",
    "# for the computational ease\n",
    "chunk_size = 25\n",
    "remainder = (len(transactions_list) % chunk_size > 0)\n",
    "num_of_chunks = len(transactions_list) // chunk_size +remainder\n",
    "frequent_variants_all = pd.DataFrame( columns=['support', 'itemsets'])\n",
    "\n",
    "# Process chunks by looping\n",
    "for i in range(num_of_chunks):\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = min((i + 1) * chunk_size, len(transactions_list))\n",
    "    transactions_chunk = transactions_list[start_idx:end_idx]\n",
    "    \n",
    "    # Mine frequent varints for the current chunk using FP-Growth algorithm\n",
    "    print(\"Processing chunk\", i+1, \" of \", num_of_chunks, \"...\")\n",
    "    frequent_variants_chunk = FPGrowthHandler.mine_frequent_variants(transactions_chunk, min_support=0.1)\n",
    "    \n",
    "    # Merge frequent variants with previous chunks\n",
    "    frequent_variants_all = pd.concat([frequent_variants_all, frequent_itemsets_chunk])\n",
    "    print(\"Mining finished for chunk\", i+1, \" of \", num_of_chunks, \"...\")\n",
    "\n",
    "# Remove duplicates in combined frequent variantss\n",
    "frequent_variants_all = frequent_variants_all.groupby('itemsets').agg({'support':'sum'}).reset_index()\n",
    "# Sort variants\n",
    "frequent_variants_final = frequent_variants_all.sort_values(by='support', ascending=False)\n",
    "\n",
    "print(\"Full Frequent Variants List: \")\n",
    "frequent_variants_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from FrequentPatternMining import AprioriHandler, TransactionManager\n",
    "\n",
    "\n",
    "# GEt the list of transactions based on events in each trace\n",
    "transactions_list = TransactionManager.create_transactions(hospital_log_cleansed)\n",
    "\n",
    "# Split transactions list into chunks \n",
    "# for the computational ease\n",
    "chunk_size = 5\n",
    "remainder = (len(transactions_list) % chunk_size > 0)\n",
    "num_of_chunks = len(transactions_list) // chunk_size + remainder\n",
    "frequent_variants_all = pd.DataFrame( columns=['support', 'itemsets'])\n",
    "\n",
    "# Process chunks by looping\n",
    "for i in range(num_of_chunks):\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = min((i + 1) * chunk_size, len(transactions_list))\n",
    "    transactions_chunk = transactions_list[start_idx:end_idx]\n",
    "    \n",
    "    # Mine frequent variantsa for the current chunk using Apriori algorithm\n",
    "    print(\"Processing chunk\", i+1, \" of \", num_of_chunks, \"...\")\n",
    "    frequent_variants_chunk = AprioriHandler.mine_frequent_variants(transactions_chunk, min_support=0.1)\n",
    "    \n",
    "    # Merge frequent varints with previous chunks\n",
    "    frequent_variants_all = pd.concat([frequent_variants_all, frequent_itemsets_chunk])\n",
    "    print(\"Mining finished for chunk\", i+1, \" of \", num_of_chunks, \"...\")\n",
    "\n",
    "# Remove duplicates in combined frequent variantss\n",
    "frequent_variants_all = frequent_variants_all.groupby('itemsets').agg({'support': 'sum'}).reset_index()\n",
    "# Sort variants\n",
    "frequent_variants_all = frequent_variants_all.sort_values(by='support', ascending=False)\n",
    "\n",
    "print(\"Full Frequent Variants List: \")\n",
    "frequent_variants_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct method (with no ML)\n",
    "\n",
    "Since it was unable to find the final list of frequent flow variants using both the above FP-Growth & the Apriori algorithms,\n",
    "now we go for a direct text mapping approach without any machine learning approach involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from FrequentPatternMining import DirectPatternMatchHandler\n",
    "\n",
    "\n",
    "# Mine frequent variants using Direct text match\n",
    "event_flows_with_counts_df = DirectPatternMatchHandler.mine_frequent_variants(hospital_log_cleansed, sort_order=\"DESC\")\n",
    "\n",
    "\n",
    "print(\"Full Frequent Variants List: \")\n",
    "event_flows_with_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_flows_with_counts_df.to_csv(\"Data\\\\Processed\\\\EventFlowsWithCounts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # Extracting the event flow and counts\n",
    "# event_flow = event_flows_with_counts_df['Event Flow']\n",
    "# counts = event_flows_with_counts_df['Count']\n",
    "\n",
    "# Plotting the distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(len(event_flows_with_counts_df['Event Flow'])), event_flows_with_counts_df['Count'], color='skyblue', linestyle='-')\n",
    "\n",
    "plt.ylabel('Flow Variant Count')\n",
    "plt.title('Distribution of Event Flow Varients ')\n",
    "plt.xticks([]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traces with their event flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from FrequentPatternMining import DirectPatternMatchHandler\n",
    "\n",
    "\n",
    "# Get the traces along with theri flow (for future usage)\n",
    "trace_event_flows_df = DirectPatternMatchHandler.get_trace_variants(hospital_log_cleansed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_event_flows_df[trace_event_flows_df['Event Flow']==('verlosk.-gynaec.   jaarkaart kosten-out', 'vervolgconsult poliklinisch', 'administratief tarief       - eerste pol')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_event_flows_df.to_csv(\"Data\\\\Processed\\\\Trace_Event_Flows.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``When starting from middle (flow variants with counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "event_flows_with_counts_df = pd.read_csv(\"Data\\\\Processed\\\\EventFlowsWithCounts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_flows_with_counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set weights for each event flow variant based on its frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Clustering import ClusterUtil\n",
    "\n",
    "# Assign weights for each variant\n",
    "event_flows_with_counts_df = ClusterUtil.assign_weights(event_flows_with_counts_df1)\n",
    "\n",
    "event_flows_with_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_flows_with_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_flows_with_counts_df.to_csv('Data\\\\Processed\\\\FlowVariantsWithWeights.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign a unique variant ID for ease of process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Clustering import ClusterUtil\n",
    "\n",
    "# Assign a unque variant id for each flow \n",
    "# focusing plotting later to avoid huge texts\n",
    "event_flows_with_counts_df = ClusterUtil.assign_unique_variant_ids(event_flows_with_counts_df, col_name='Variant No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_flows_with_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_flows_with_counts_df.to_csv('Data\\\\Processed\\\\FlowVariantsWithWeightsAndVariantNo.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualifying percentage of most frequent flow variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 66% of the frequent flow variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the index up 66% of recrods\n",
    "all_records = len(event_flows_with_counts_df)\n",
    "index_66 = int(all_records * 0.66)\n",
    "\n",
    "# Get the first 66% of the records\n",
    "event_flows_with_counts_df_66 = event_flows_with_counts_df.head(index_66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_flows_with_counts_df_66"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50% of the frequent flow variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the index up 50% of recrods\n",
    "all_records = len(event_flows_with_counts_df)\n",
    "index_50 = int(all_records * 0.5)\n",
    "\n",
    "# Get the first 50% of the records\n",
    "event_flows_with_counts_df_50 = event_flows_with_counts_df.head(index_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_flows_with_counts_df_50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 33% of the frequent flow variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the index up 330% of recrods\n",
    "all_records = len(event_flows_with_counts_df)\n",
    "index_33 = int(all_records * 0.33)\n",
    "\n",
    "# Get the first 33% of the records\n",
    "event_flows_with_counts_df_33 = event_flows_with_counts_df.head(index_33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_flows_with_counts_df_33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "Here we focus to perform Agglomerative Heirachical clustering with average linkage method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 66% of frequent flow variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate distances among flow variant pairs\n",
    "\n",
    "Here we use Jaccard Similaritiy method for this purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Clustering import ClusterHandler\n",
    "\n",
    "# Calculate Jaccard similarity for flows\n",
    "pairwise_distance_list_66, event_binary_matrix_np_66 = ClusterHandler.calculate_distances_with_jaccard(event_flows_with_counts_df_66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_distance_list_66"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Agglomerative Hierarchical Clustering\n",
    "\n",
    "Here we use the average linkage method which considers distance among all other nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Clustering import ClusterHandler\n",
    "\n",
    "# Perform Agglomerative Hierarchical clustering with Average Linkage method\n",
    "clustered_flows_66 = ClusterHandler.perform_hierarchical_clustering(pairwise_distance_list_66, method='average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_flows_66"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate dendogram for the heirachical tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Clustering import ClusterHandler\n",
    "\n",
    "# plot & save the dendogram\n",
    "ClusterHandler.generate_dendogram(clustered_flows_66, event_flows_with_counts_df_66,\n",
    "                                        title='Agglomerative Hierarchical Clustering Dendrogram - 66 percent of variants',\n",
    "                                        col_name='Variant No',\n",
    "                                        xlabel= 'Event Flow Variants', \n",
    "                                        ylabel= 'Distance', \n",
    "                                        save_location= 'Visualize\\\\HierarchicalClusteringDendrogram66.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding optimal number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Silhouette Scores\n",
    "\n",
    "Optimum number of clusters decided based on the maximum Silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Clustering import ClusterHandler\n",
    "\n",
    "# plot silhouett scores & return the optimum cluster number\n",
    "opt_num_of_clusters_66 = ClusterHandler.plot_silhouette_scores(clustered_flows_66, event_binary_matrix_np_66)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Elbow method\n",
    "\n",
    "Here we find the optimum number of clusters from where we find the elbow shape in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Clustering import ClusterHandler\n",
    "\n",
    "# plot elbow method values \n",
    "ClusterHandler.plot_elbow_method(clustered_flows_66, event_binary_matrix_np_66)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster variants based on their weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_flows_with_counts_df_66"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide into 2 Clusters based on Silhouette analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Clustering import ClusterHandler\n",
    "\n",
    "# Cluster based on weights\n",
    "all_clusters_66 = ClusterHandler.form_cluster_variants(event_flows_with_counts_df_66, clustered_flows_66, num_of_clusters=2, max_variants=200)\n",
    "cluster_66_1, cluster_66_2 = all_clusters_66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cluster_66_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cluster_66_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Clustering import ClusterUtil\n",
    "\n",
    "# Write each cluster as a csv to disk\n",
    "ClusterUtil.write_cluster_to_csv(cluster_66_1,\"Data\\\\Processed\\\\Cluster66_Sil_1.csv\")\n",
    "ClusterUtil.write_cluster_to_csv(cluster_66_2,\"Data\\\\Processed\\\\Cluster66_Sil_2.csv\")\n",
    "#ClusterUtil.write_cluster_to_csv(cluster3,\"Data\\\\Processed\\\\Cluster3_Event_Log.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide into 4 Clusters based on Elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Clustering import ClusterHandler\n",
    "\n",
    "# Cluster based on weights\n",
    "all_clusters_66 = ClusterHandler.form_cluster_variants(event_flows_with_counts_df_66, clustered_flows_66, num_of_clusters=4, max_variants=200)\n",
    "cluster_66_Elbow_1, cluster_66_Elbow_2, cluster_66_Elbow_3, cluster_66_Elbow_4 = all_clusters_66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cluster_66_Elbow_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cluster_66_Elbow_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cluster_66_Elbow_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cluster_66_Elbow_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50% of frequent flow variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate distances among flow variant pairs\n",
    "\n",
    "Here we use Jaccard Similaritiy method for this purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Clustering import ClusterHandler\n",
    "\n",
    "# Calculate Jaccard similarity for flows\n",
    "pairwise_distance_list_50, event_binary_matrix_np_50 = ClusterHandler.calculate_distances_with_jaccard(event_flows_with_counts_df_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_distance_list_50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Agglomerative Hierarchical Clustering\n",
    "\n",
    "Here we use the average linkage method which considers distance among all other nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Clustering import ClusterHandler\n",
    "\n",
    "# Perform Agglomerative Hierarchical clustering with Average Linkage method\n",
    "clustered_flows_50 = ClusterHandler.perform_hierarchical_clustering(pairwise_distance_list_50, method='average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_flows_50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate dendrogram for the heirachical tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Clustering import ClusterHandler\n",
    "\n",
    "# plot & save the dendogram\n",
    "ClusterHandler.generate_dendogram(clustered_flows_50, event_flows_with_counts_df_50,\n",
    "                                        title='Agglomerative Hierarchical Clustering Dendrogram - 50 percent of variants',\n",
    "                                        col_name='Variant No',\n",
    "                                        xlabel= 'Event Flow Variants', \n",
    "                                        ylabel= 'Distance', \n",
    "                                        save_location= 'Visualize\\\\HierarchicalClusteringDendrogram50.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding optimal number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Silhouette Scores\n",
    "\n",
    "Optimum number of clusters decided based on the maximum Silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Clustering import ClusterHandler\n",
    "\n",
    "# plot silhouett scores & return the optimum cluster number\n",
    "opt_num_of_clusters_50 = ClusterHandler.plot_silhouette_scores(clustered_flows_50, event_binary_matrix_np_50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Elbow method\n",
    "\n",
    "Here we find the optimum number of clusters from where we find the elbow shape in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Clustering import ClusterHandler\n",
    "\n",
    "# plot elbow method values \n",
    "ClusterHandler.plot_elbow_method(clustered_flows_50, event_binary_matrix_np_50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster variants based on their weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_flows_with_counts_df_50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide into 5 Clusters based on Elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Clustering import ClusterHandler\n",
    "\n",
    "# Cluster based on weights\n",
    "all_clusters_50 = ClusterHandler.form_cluster_variants(event_flows_with_counts_df_50, clustered_flows_50, num_of_clusters=5, max_variants=125)\n",
    "cluster_50_1, cluster_50_2, cluster_50_3, cluster_50_4, cluster_50_5 = all_clusters_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cluster_50_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cluster_50_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cluster_50_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cluster_50_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cluster_50_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 33% of frequent flow variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate distances among flow variant pairs\n",
    "\n",
    "Here we use Jaccard Similaritiy method for this purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Clustering import ClusterHandler\n",
    "\n",
    "# Calculate Jaccard similarity for flows\n",
    "pairwise_distance_list_33, event_binary_matrix_np_33 = ClusterHandler.calculate_distances_with_jaccard(event_flows_with_counts_df_33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Agglomerative Hierarchical Clustering\n",
    "\n",
    "Here we use the average linkage method which considers distance among all other nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Clustering import ClusterHandler\n",
    "\n",
    "# Perform Agglomerative Hierarchical clustering with Average Linkage method\n",
    "clustered_flows_33 = ClusterHandler.perform_hierarchical_clustering(pairwise_distance_list_33, method='average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_flows_33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate dendrogram for the heirachical tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Clustering import ClusterHandler\n",
    "\n",
    "# plot & save the dendogram\n",
    "ClusterHandler.generate_dendogram(clustered_flows_33, event_flows_with_counts_df_33,\n",
    "                                        title='Agglomerative Hierarchical Clustering Dendrogram - 33 percent of variants',\n",
    "                                        col_name='Variant No',\n",
    "                                        xlabel= 'Event Flow Variants', \n",
    "                                        ylabel= 'Distance', \n",
    "                                        save_location= 'Visualize\\\\HierarchicalClusteringDendrogram33.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding optimal number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Silhouette Scores\n",
    "\n",
    "Optimum number of clusters decided based on the maximum Silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Clustering import ClusterHandler\n",
    "\n",
    "# plot silhouett scores & return the optimum cluster number\n",
    "opt_num_of_clusters_33 = ClusterHandler.plot_silhouette_scores(clustered_flows_33, event_binary_matrix_np_33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Elbow method\n",
    "\n",
    "Here we find the optimum number of clusters from where we find the elbow shape in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Clustering import ClusterHandler\n",
    "\n",
    "# plot elbow method values \n",
    "ClusterHandler.plot_elbow_method(clustered_flows_33, event_binary_matrix_np_33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster variants based on their weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_flows_with_counts_df_33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide into 5 Clusters based on Elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Clustering import ClusterHandler\n",
    "\n",
    "# Cluster based on weights\n",
    "all_clusters_33 = ClusterHandler.form_cluster_variants(event_flows_with_counts_df_33, clustered_flows_33, num_of_clusters=5)\n",
    "cluster_33_1, cluster_33_2, cluster_33_3, cluster_33_4, cluster_33_5 = all_clusters_33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cluster_33_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cluster_33_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cluster_33_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cluster_33_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cluster_33_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_event_flows_df = pd.read_csv(\"Data\\\\Processed\\\\Trace_Event_Flows.csv\", dtype={'Case ID': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_event_flows_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Discovery - Full Event Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the process model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProcessDiscovery import ProcessDiscoveryHandler\n",
    "\n",
    "hospital_log_full_process_model = ProcessDiscoveryHandler.discover_process(hospital_log_cleansed, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discover the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 66% of frequent flow variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With 2 clusters (Silhouette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Discovery - Cluster1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare final event log for the cluster using actual trace records for process discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_event_flows_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProcessDiscovery import ProcessDiscoveryHandler\n",
    "\n",
    "# Create the event log (with all attributes) for the cluster\n",
    "cluster_66_1_event_log_actual_df = ProcessDiscoveryHandler.create_cluster_full_event_log(cluster_66_1, trace_event_flows_df, hospital_log_cleansed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_66_1_event_log_actual_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discover the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProcessDiscovery import ProcessDiscoveryHandler\n",
    "\n",
    "# Discover the process for the cluster log\n",
    "cluster_66_1_processs_model = ProcessDiscoveryHandler.discover_process(cluster_66_1_event_log_actual_df, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_66_1_processs_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualize the discovered process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the process for the cluster log\n",
    "save_file = 'Visualize\\\\ProcessDiscovery\\\\Percent66\\\\Cluster_66_Sil_1_Process_Visualization.png'\n",
    "ProcessDiscoveryHandler.visualize_process(cluster_66_1_processs_model, save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Discovery - Cluster2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare final event log for the cluster using actual trace records for process discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProcessDiscovery import ProcessDiscoveryHandler\n",
    "\n",
    "# Create the event log (with all attributes) for the cluster\n",
    "cluster_66_2_event_log_actual_df = ProcessDiscoveryHandler.create_cluster_full_event_log(cluster_66_2, trace_event_flows_df, hospital_log_cleansed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_66_2_event_log_actual_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discover the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProcessDiscovery import ProcessDiscoveryHandler\n",
    "\n",
    "# Discover the process for the cluster log\n",
    "cluster_66_2_processs_model = ProcessDiscoveryHandler.discover_process(cluster_66_2_event_log_actual_df, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the process for the cluster log\n",
    "save_file = 'Visualize\\\\ProcessDiscovery\\\\Percent66\\\\Cluster_66_Sil_2_Process_Visualization.png'\n",
    "ProcessDiscoveryHandler.visualize_process(cluster_66_2_processs_model, save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With 4 clusters (Elbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Discovery - Cluster1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare final event log for the cluster using actual trace records for process discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProcessDiscovery import ProcessDiscoveryHandler\n",
    "\n",
    "# Create the event log (with all attributes) for the cluster\n",
    "cluster_66_Elbow_1_event_log_actual_df = ProcessDiscoveryHandler.create_cluster_full_event_log(cluster_66_Elbow_1, trace_event_flows_df, hospital_log_cleansed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_66_Elbow_1_event_log_actual_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discover the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProcessDiscovery import ProcessDiscoveryHandler\n",
    "\n",
    "# Discover the process for the cluster log\n",
    "cluster_66_Elbow_1_processs_model = ProcessDiscoveryHandler.discover_process(cluster_66_Elbow_1_event_log_actual_df, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Discovery - Cluster2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare final event log for the cluster using actual trace records for process discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProcessDiscovery import ProcessDiscoveryHandler\n",
    "\n",
    "# Create the event log (with all attributes) for the cluster\n",
    "cluster_66_Elbow_2_event_log_actual_df = ProcessDiscoveryHandler.create_cluster_full_event_log(cluster_66_Elbow_2, trace_event_flows_df, hospital_log_cleansed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_66_Elbow_2_event_log_actual_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discover the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProcessDiscovery import ProcessDiscoveryHandler\n",
    "\n",
    "# Discover the process for the cluster log\n",
    "cluster_66_Elbow_2_processs_model = ProcessDiscoveryHandler.discover_process(cluster_66_Elbow_2_event_log_actual_df, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Discovery - Cluster3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare final event log for the cluster using actual trace records for process discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProcessDiscovery import ProcessDiscoveryHandler\n",
    "\n",
    "# Create the event log (with all attributes) for the cluster\n",
    "cluster_66_Elbow_3_event_log_actual_df = ProcessDiscoveryHandler.create_cluster_full_event_log(cluster_66_Elbow_3, trace_event_flows_df, hospital_log_cleansed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_66_Elbow_3_event_log_actual_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discover the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProcessDiscovery import ProcessDiscoveryHandler\n",
    "\n",
    "# Discover the process for the cluster log\n",
    "cluster_66_Elbow_3_processs_model = ProcessDiscoveryHandler.discover_process(cluster_66_Elbow_3_event_log_actual_df, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Discovery - Cluster4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare final event log for the cluster using actual trace records for process discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProcessDiscovery import ProcessDiscoveryHandler\n",
    "\n",
    "# Create the event log (with all attributes) for the cluster\n",
    "cluster_66_Elbow_4_event_log_actual_df = ProcessDiscoveryHandler.create_cluster_full_event_log(cluster_66_Elbow_4, trace_event_flows_df, hospital_log_cleansed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_66_Elbow_4_event_log_actual_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discover the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProcessDiscovery import ProcessDiscoveryHandler\n",
    "\n",
    "# Discover the process for the cluster log\n",
    "cluster_66_Elbow_4_processs_model = ProcessDiscoveryHandler.discover_process(cluster_66_Elbow_4_event_log_actual_df, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50% of frequent flow variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With 5 clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Discovery - Cluster1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare final event log for the cluster using actual trace records for process discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProcessDiscovery import ProcessDiscoveryHandler\n",
    "\n",
    "# Create the event log (with all attributes) for the cluster\n",
    "cluster_50_1_event_log_actual_df = ProcessDiscoveryHandler.create_cluster_full_event_log(cluster_50_1, trace_event_flows_df, hospital_log_cleansed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_50_1_event_log_actual_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discover the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProcessDiscovery import ProcessDiscoveryHandler\n",
    "\n",
    "# Discover the process for the cluster log\n",
    "cluster_50_1_processs_model = ProcessDiscoveryHandler.discover_process(cluster_50_1_event_log_actual_df, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Discovery - Cluster2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare final event log for the cluster using actual trace records for process discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProcessDiscovery import ProcessDiscoveryHandler\n",
    "\n",
    "# Create the event log (with all attributes) for the cluster\n",
    "cluster_50_2_event_log_actual_df = ProcessDiscoveryHandler.create_cluster_full_event_log(cluster_50_2, trace_event_flows_df, hospital_log_cleansed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_50_2_event_log_actual_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discover the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProcessDiscovery import ProcessDiscoveryHandler\n",
    "\n",
    "# Discover the process for the cluster log\n",
    "cluster_50_2_processs_model = ProcessDiscoveryHandler.discover_process(cluster_50_2_event_log_actual_df, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Discovery - Cluster3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare final event log for the cluster using actual trace records for process discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProcessDiscovery import ProcessDiscoveryHandler\n",
    "\n",
    "# Create the event log (with all attributes) for the cluster\n",
    "cluster_50_3_event_log_actual_df = ProcessDiscoveryHandler.create_cluster_full_event_log(cluster_50_3, trace_event_flows_df, hospital_log_cleansed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_50_3_event_log_actual_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discover the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProcessDiscovery import ProcessDiscoveryHandler\n",
    "\n",
    "# Discover the process for the cluster log\n",
    "cluster_50_3_processs_model = ProcessDiscoveryHandler.discover_process(cluster_50_3_event_log_actual_df, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Discovery - Cluster4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare final event log for the cluster using actual trace records for process discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProcessDiscovery import ProcessDiscoveryHandler\n",
    "\n",
    "# Create the event log (with all attributes) for the cluster\n",
    "cluster_50_4_event_log_actual_df = ProcessDiscoveryHandler.create_cluster_full_event_log(cluster_50_4, trace_event_flows_df, hospital_log_cleansed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_50_4_event_log_actual_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discover the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProcessDiscovery import ProcessDiscoveryHandler\n",
    "\n",
    "# Discover the process for the cluster log\n",
    "cluster_50_4_processs_model = ProcessDiscoveryHandler.discover_process(cluster_50_4_event_log_actual_df, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Discovery - Cluster5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare final event log for the cluster using actual trace records for process discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProcessDiscovery import ProcessDiscoveryHandler\n",
    "\n",
    "# Create the event log (with all attributes) for the cluster\n",
    "cluster_50_5_event_log_actual_df = ProcessDiscoveryHandler.create_cluster_full_event_log(cluster_50_5, trace_event_flows_df, hospital_log_cleansed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_50_5_event_log_actual_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discover the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProcessDiscovery import ProcessDiscoveryHandler\n",
    "\n",
    "# Discover the process for the cluster log\n",
    "cluster_50_5_processs_model = ProcessDiscoveryHandler.discover_process(cluster_50_5_event_log_actual_df, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 33% of frequent flow variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With 5 clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Discovery - Cluster1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare final event log for the cluster using actual trace records for process discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProcessDiscovery import ProcessDiscoveryHandler\n",
    "\n",
    "# Create the event log (with all attributes) for the cluster\n",
    "cluster_33_1_event_log_actual_df = ProcessDiscoveryHandler.create_cluster_full_event_log(cluster_33_1, trace_event_flows_df, hospital_log_cleansed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_33_1_event_log_actual_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discover the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProcessDiscovery import ProcessDiscoveryHandler\n",
    "\n",
    "# Discover the process for the cluster log\n",
    "cluster_33_1_processs_model = ProcessDiscoveryHandler.discover_process(cluster_33_1_event_log_actual_df, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Discovery - Cluster2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare final event log for the cluster using actual trace records for process discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProcessDiscovery import ProcessDiscoveryHandler\n",
    "\n",
    "# Create the event log (with all attributes) for the cluster\n",
    "cluster_33_2_event_log_actual_df = ProcessDiscoveryHandler.create_cluster_full_event_log(cluster_33_2, trace_event_flows_df, hospital_log_cleansed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_33_2_event_log_actual_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discover the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProcessDiscovery import ProcessDiscoveryHandler\n",
    "\n",
    "# Discover the process for the cluster log\n",
    "cluster_33_2_processs_model = ProcessDiscoveryHandler.discover_process(cluster_33_2_event_log_actual_df, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Discovery - Cluster3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare final event log for the cluster using actual trace records for process discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProcessDiscovery import ProcessDiscoveryHandler\n",
    "\n",
    "# Create the event log (with all attributes) for the cluster\n",
    "cluster_33_3_event_log_actual_df = ProcessDiscoveryHandler.create_cluster_full_event_log(cluster_33_3, trace_event_flows_df, hospital_log_cleansed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_33_3_event_log_actual_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discover the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProcessDiscovery import ProcessDiscoveryHandler\n",
    "\n",
    "# Discover the process for the cluster log\n",
    "cluster_33_3_processs_model = ProcessDiscoveryHandler.discover_process(cluster_33_3_event_log_actual_df, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Discovery - Cluster4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare final event log for the cluster using actual trace records for process discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProcessDiscovery import ProcessDiscoveryHandler\n",
    "\n",
    "# Create the event log (with all attributes) for the cluster\n",
    "cluster_33_4_event_log_actual_df = ProcessDiscoveryHandler.create_cluster_full_event_log(cluster_33_4, trace_event_flows_df, hospital_log_cleansed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_33_4_event_log_actual_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discover the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProcessDiscovery import ProcessDiscoveryHandler\n",
    "\n",
    "# Discover the process for the cluster log\n",
    "cluster_33_4_processs_model = ProcessDiscoveryHandler.discover_process(cluster_33_4_event_log_actual_df, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Discovery - Cluster5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare final event log for the cluster using actual trace records for process discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProcessDiscovery import ProcessDiscoveryHandler\n",
    "\n",
    "# Create the event log (with all attributes) for the cluster\n",
    "cluster_33_5_event_log_actual_df = ProcessDiscoveryHandler.create_cluster_full_event_log(cluster_33_5, trace_event_flows_df, hospital_log_cleansed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_33_5_event_log_actual_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discover the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ProcessDiscovery import ProcessDiscoveryHandler\n",
    "\n",
    "# Discover the process for the cluster log\n",
    "cluster_33_5_processs_model = ProcessDiscoveryHandler.discover_process(cluster_33_5_event_log_actual_df, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conformance Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConformanceChecking import ConformanceCheckingHandler\n",
    "\n",
    "# calculating the fitness & the precision of full log against full process model\n",
    "full_log_fitness, full_log_precision = ConformanceCheckingHandler.get_conformance(hospital_log_full_process_model, hospital_log_cleansed, True)\n",
    "\n",
    "print ('Fitness: ', full_log_fitness)\n",
    "print ('Precision: ', full_log_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 66% of frequent flow variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With 2 clusters (Silhouette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conformance Checking - Cluster 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the conformance of the cluster1 log with its own process model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConformanceChecking import ConformanceCheckingHandler\n",
    "\n",
    "# calculating the fitness & the precision of cluster 1 against own process model\n",
    "cluster_66_1_own_fitness, cluster_66_1__own_precision = ConformanceCheckingHandler.get_conformance(cluster_66_1_processs_model, cluster_66_1_event_log_actual_df, False)\n",
    "\n",
    "print ('Conformance of cluster 1 against own process model')\n",
    "print ('--------------------------------------------------')\n",
    "print ('Fitness: ', cluster_66_1_own_fitness)\n",
    "print ('Precision: ', cluster_66_1__own_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the conformance of the cluster1 log with the process model of the full event log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConformanceChecking import ConformanceCheckingHandler\n",
    "\n",
    "# calculating the fitness & the precision of cluster 1 against full process model\n",
    "cluster_66_1_full_fitness, cluster_66_1_full_precision = ConformanceCheckingHandler.get_conformance(hospital_log_full_process_model, cluster_66_1_event_log_actual_df, False)\n",
    "\n",
    "print ('Conformance of cluster 1 against full process model')\n",
    "print ('---------------------------------------------------')\n",
    "print ('Fitness: ', cluster_66_1_full_fitness)\n",
    "print ('Precision: ', cluster_66_1_full_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conformance Checking - Cluster 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the conformance of the cluster 2 log with its own process model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConformanceChecking import ConformanceCheckingHandler\n",
    "\n",
    "# calculating the fitness & the precision of cluster 2 against full process model\n",
    "cluster_66_2_own_fitness, cluster_66_2_own_precision = ConformanceCheckingHandler.get_conformance(cluster_66_2_processs_model, cluster_66_2_event_log_actual_df, False)\n",
    "\n",
    "print ('Conformance of cluster 2 against own process model')\n",
    "print ('--------------------------------------------------')\n",
    "print ('Fitness: ', cluster_66_2_own_fitness)\n",
    "print ('Precision: ', cluster_66_2_own_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the conformance of the cluster 2 log against the process model of full event log, discovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConformanceChecking import ConformanceCheckingHandler\n",
    "\n",
    "# calculating the fitness & the precision of cluster 2 against full process model\n",
    "cluster_66_2_full_fitness, cluster_66_2_full_precision = ConformanceCheckingHandler.get_conformance(hospital_log_full_process_model, cluster_66_2_event_log_actual_df, False)\n",
    "\n",
    "print ('Conformance of cluster 2 against full process model')\n",
    "print ('---------------------------------------------------')\n",
    "print ('Fitness: ', cluster_66_2_full_fitness)\n",
    "print ('Precision: ', cluster_66_2_full_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With 4 clusters (Elbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conformance Checking - Cluster 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the conformance of the cluster 1 log with its own process model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConformanceChecking import ConformanceCheckingHandler\n",
    "\n",
    "# calculating the fitness & the precision of cluster 1 against own process model\n",
    "cluster_66_elbow_1_own_fitness, cluster_66_elbow_1_own_precision = ConformanceCheckingHandler.get_conformance(cluster_66_Elbow_1_processs_model, cluster_66_Elbow_1_event_log_actual_df, False)\n",
    "\n",
    "print ('Conformance of cluster 1 against own process model')\n",
    "print ('--------------------------------------------------')\n",
    "print ('Fitness: ', cluster_66_elbow_1_own_fitness)\n",
    "print ('Precision: ', cluster_66_elbow_1_own_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the conformance of the cluster 1 log against the process model of full event log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConformanceChecking import ConformanceCheckingHandler\n",
    "\n",
    "# calculating the fitness & the precision of cluster 1 against full process model\n",
    "cluster_66_elbow_1_full_fitness, cluster_66_elbow_1_full_precision = ConformanceCheckingHandler.get_conformance(hospital_log_full_process_model, cluster_66_Elbow_1_event_log_actual_df, False)\n",
    "\n",
    "print ('Conformance of cluster 1 against full process model')\n",
    "print ('---------------------------------------------------')\n",
    "print ('Fitness: ', cluster_66_elbow_1_full_fitness)\n",
    "print ('Precision: ', cluster_66_elbow_1_full_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conformance Checking - Cluster 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the conformance of the cluster 2 log with its own process model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConformanceChecking import ConformanceCheckingHandler\n",
    "\n",
    "# calculating the fitness & the precision of cluster 2 against own process model\n",
    "cluster_66_elbow_2_own_fitness, cluster_66_elbow_2_own_precision = ConformanceCheckingHandler.get_conformance(cluster_66_Elbow_2_processs_model, cluster_66_Elbow_2_event_log_actual_df, False)\n",
    "\n",
    "print ('Conformance of cluster 2 against own process model')\n",
    "print ('--------------------------------------------------')\n",
    "print ('Fitness: ', cluster_66_elbow_2_own_fitness)\n",
    "print ('Precision: ', cluster_66_elbow_2_own_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the conformance of the cluster 2 log against the process model of full event log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConformanceChecking import ConformanceCheckingHandler\n",
    "\n",
    "# calculating the fitness & the precision of cluster 2 against full process model\n",
    "cluster_66_elbow_2_full_fitness, cluster_66_elbow_2_full_precision = ConformanceCheckingHandler.get_conformance(hospital_log_full_process_model, cluster_66_Elbow_2_event_log_actual_df, False)\n",
    "\n",
    "print ('Conformance of cluster 2 against full process model')\n",
    "print ('---------------------------------------------------')\n",
    "print ('Fitness: ', cluster_66_elbow_2_full_fitness)\n",
    "print ('Precision: ', cluster_66_elbow_2_full_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conformance Checking - Cluster 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the conformance of the cluster 3 log with its own process model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConformanceChecking import ConformanceCheckingHandler\n",
    "\n",
    "# calculating the fitness & the precision of cluster 3 against own process model\n",
    "cluster_66_elbow_3_own_fitness, cluster_66_elbow_3_own_precision = ConformanceCheckingHandler.get_conformance(cluster_66_Elbow_3_processs_model, cluster_66_Elbow_3_event_log_actual_df, False)\n",
    "\n",
    "print ('Conformance of cluster 3 against own process model')\n",
    "print ('--------------------------------------------------')\n",
    "print ('Fitness: ', cluster_66_elbow_3_own_fitness)\n",
    "print ('Precision: ', cluster_66_elbow_3_own_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the conformance of the cluster 3 log against the process model of full event log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConformanceChecking import ConformanceCheckingHandler\n",
    "\n",
    "# calculating the fitness & the precision of cluster 3 against full process model\n",
    "cluster_66_elbow_3_full_fitness, cluster_66_elbow_3_full_precision = ConformanceCheckingHandler.get_conformance(hospital_log_full_process_model, cluster_66_Elbow_3_event_log_actual_df, False)\n",
    "\n",
    "print ('Conformance of cluster 3 against full process model')\n",
    "print ('---------------------------------------------------')\n",
    "print ('Fitness: ', cluster_66_elbow_3_full_fitness)\n",
    "print ('Precision: ', cluster_66_elbow_3_full_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conformance Checking - Cluster 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the conformance of the cluster 4 log with its own process model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConformanceChecking import ConformanceCheckingHandler\n",
    "\n",
    "# calculating the fitness & the precision of cluster 4 against own process model\n",
    "cluster_66_elbow_4_own_fitness, cluster_66_elbow_4_own_precision = ConformanceCheckingHandler.get_conformance(cluster_66_Elbow_4_processs_model, cluster_66_Elbow_4_event_log_actual_df, False)\n",
    "\n",
    "print ('Conformance of cluster 4 against own process model')\n",
    "print ('--------------------------------------------------')\n",
    "print ('Fitness: ', cluster_66_elbow_4_own_fitness)\n",
    "print ('Precision: ', cluster_66_elbow_4_own_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the conformance of the cluster 4 log against the process model of full event log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConformanceChecking import ConformanceCheckingHandler\n",
    "\n",
    "# calculating the fitness & the precision of cluster 4 against full process model\n",
    "cluster_66_elbow_4_full_fitness, cluster_66_elbow_4_full_precision = ConformanceCheckingHandler.get_conformance(hospital_log_full_process_model, cluster_66_Elbow_4_event_log_actual_df, False)\n",
    "\n",
    "print ('Conformance of cluster 4 against full process model')\n",
    "print ('---------------------------------------------------')\n",
    "print ('Fitness: ', cluster_66_elbow_4_full_fitness)\n",
    "print ('Precision: ', cluster_66_elbow_4_full_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50% of frequent flow variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With 5 clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conformance Checking - Cluster 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the conformance of the cluster 1 log with its own process model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConformanceChecking import ConformanceCheckingHandler\n",
    "\n",
    "# calculating the fitness & the precision of cluster 1 against own process model\n",
    "cluster_50_1_own_fitness, cluster_50_1_own_precision = ConformanceCheckingHandler.get_conformance(cluster_50_1_processs_model, cluster_50_1_event_log_actual_df, False)\n",
    "\n",
    "print ('Conformance of cluster 1 against own process model')\n",
    "print ('--------------------------------------------------')\n",
    "print ('Fitness: ', cluster_50_1_own_fitness)\n",
    "print ('Precision: ', cluster_50_1_own_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the conformance of the cluster 1 log with the process model of the full event log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConformanceChecking import ConformanceCheckingHandler\n",
    "\n",
    "# calculating the fitness & the precision of cluster 1 against full process model\n",
    "cluster_50_1_full_fitness, cluster_50_1_full_precision = ConformanceCheckingHandler.get_conformance(hospital_log_full_process_model, cluster_50_1_event_log_actual_df, False)\n",
    "\n",
    "print ('Conformance of cluster 1 against full process model')\n",
    "print ('---------------------------------------------------')\n",
    "print ('Fitness: ', cluster_50_1_full_fitness)\n",
    "print ('Precision: ', cluster_50_1_full_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conformance Checking - Cluster 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the conformance of the cluster 2 log with its own process model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConformanceChecking import ConformanceCheckingHandler\n",
    "\n",
    "# calculating the fitness & the precision of cluster 2 against own process model\n",
    "cluster_50_2_own_fitness, cluster_50_2_own_precision = ConformanceCheckingHandler.get_conformance(cluster_50_2_processs_model, cluster_50_2_event_log_actual_df, False)\n",
    "\n",
    "print ('Conformance of cluster 2 against own process model')\n",
    "print ('--------------------------------------------------')\n",
    "print ('Fitness: ', cluster_50_2_own_fitness)\n",
    "print ('Precision: ', cluster_50_2_own_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the conformance of the cluster 2 log with the process model of the full event log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConformanceChecking import ConformanceCheckingHandler\n",
    "\n",
    "# calculating the fitness & the precision of cluster 2 against full process model\n",
    "cluster_50_2_full_fitness, cluster_50_2_full_precision = ConformanceCheckingHandler.get_conformance(hospital_log_full_process_model, cluster_50_2_event_log_actual_df, False)\n",
    "\n",
    "print ('Conformance of cluster 2 against full process model')\n",
    "print ('---------------------------------------------------')\n",
    "print ('Fitness: ', cluster_50_2_full_fitness)\n",
    "print ('Precision: ', cluster_50_2_full_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conformance Checking - Cluster 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the conformance of the cluster 3 log with its own process model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConformanceChecking import ConformanceCheckingHandler\n",
    "\n",
    "# calculating the fitness & the precision of cluster 3 against own process model\n",
    "cluster_50_3_own_fitness, cluster_50_3_own_precision = ConformanceCheckingHandler.get_conformance(cluster_50_3_processs_model, cluster_50_3_event_log_actual_df, False)\n",
    "\n",
    "print ('Conformance of cluster 3 against own process model')\n",
    "print ('--------------------------------------------------')\n",
    "print ('Fitness: ', cluster_50_3_own_fitness)\n",
    "print ('Precision: ', cluster_50_3_own_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the conformance of the cluster 3 log with the process model of the full event log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConformanceChecking import ConformanceCheckingHandler\n",
    "\n",
    "# calculating the fitness & the precision of cluster 3 against full process model\n",
    "cluster_50_3_full_fitness, cluster_50_3_full_precision = ConformanceCheckingHandler.get_conformance(hospital_log_full_process_model, cluster_50_3_event_log_actual_df, False)\n",
    "\n",
    "print ('Conformance of cluster 3 against full process model')\n",
    "print ('---------------------------------------------------')\n",
    "print ('Fitness: ', cluster_50_3_full_fitness)\n",
    "print ('Precision: ', cluster_50_3_full_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conformance Checking - Cluster 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the conformance of the cluster 4 log with its own process model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConformanceChecking import ConformanceCheckingHandler\n",
    "\n",
    "# calculating the fitness & the precision of cluster 4 against own process model\n",
    "cluster_50_4_own_fitness, cluster_50_4_own_precision = ConformanceCheckingHandler.get_conformance(cluster_50_4_processs_model, cluster_50_4_event_log_actual_df, False)\n",
    "\n",
    "print ('Conformance of cluster 4 against own process model')\n",
    "print ('--------------------------------------------------')\n",
    "print ('Fitness: ', cluster_50_4_own_fitness)\n",
    "print ('Precision: ', cluster_50_4_own_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the conformance of the cluster 4 log with the process model of the full event log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConformanceChecking import ConformanceCheckingHandler\n",
    "\n",
    "# calculating the fitness & the precision of cluster 4 against full process model\n",
    "cluster_50_4_full_fitness, cluster_50_4_full_precision = ConformanceCheckingHandler.get_conformance(hospital_log_full_process_model, cluster_50_4_event_log_actual_df, False)\n",
    "\n",
    "print ('Conformance of cluster 4 against full process model')\n",
    "print ('---------------------------------------------------')\n",
    "print ('Fitness: ', cluster_50_4_full_fitness)\n",
    "print ('Precision: ', cluster_50_4_full_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conformance Checking - Cluster 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the conformance of the cluster 5 log with its own process model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConformanceChecking import ConformanceCheckingHandler\n",
    "\n",
    "# calculating the fitness & the precision of cluster 5 against own process model\n",
    "cluster_50_5_own_fitness, cluster_50_5_own_precision = ConformanceCheckingHandler.get_conformance(cluster_50_5_processs_model, cluster_50_5_event_log_actual_df, False)\n",
    "\n",
    "print ('Conformance of cluster 5 against own process model')\n",
    "print ('--------------------------------------------------')\n",
    "print ('Fitness: ', cluster_50_5_own_fitness)\n",
    "print ('Precision: ', cluster_50_5_own_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the conformance of the cluster 5 log with the process model of the full event log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConformanceChecking import ConformanceCheckingHandler\n",
    "\n",
    "# calculating the fitness & the precision of cluster 5 against full process model\n",
    "cluster_50_5_full_fitness, cluster_50_5_full_precision = ConformanceCheckingHandler.get_conformance(hospital_log_full_process_model, cluster_50_5_event_log_actual_df, False)\n",
    "\n",
    "print ('Conformance of cluster 5 against full process model')\n",
    "print ('---------------------------------------------------')\n",
    "print ('Fitness: ', cluster_50_5_full_fitness)\n",
    "print ('Precision: ', cluster_50_5_full_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 33% of frequent flow variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With 5 clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conformance Checking - Cluster 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the conformance of the cluster 1 log with its own process model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConformanceChecking import ConformanceCheckingHandler\n",
    "\n",
    "# calculating the fitness & the precision of cluster 1 against own process model\n",
    "cluster_33_1_own_fitness, cluster_33_1_own_precision = ConformanceCheckingHandler.get_conformance(cluster_33_1_processs_model, cluster_33_1_event_log_actual_df, False)\n",
    "\n",
    "print ('Conformance of cluster 1 against own process model')\n",
    "print ('--------------------------------------------------')\n",
    "print ('Fitness: ', cluster_33_1_own_fitness)\n",
    "print ('Precision: ', cluster_33_1_own_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the conformance of the cluster 1 log with the process model of the full event log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConformanceChecking import ConformanceCheckingHandler\n",
    "\n",
    "# calculating the fitness & the precision of cluster 1 against full process model\n",
    "cluster_33_1_full_fitness, cluster_33_1_full_precision = ConformanceCheckingHandler.get_conformance(hospital_log_full_process_model, cluster_33_1_event_log_actual_df, False)\n",
    "\n",
    "print ('Conformance of cluster 1 against full process model')\n",
    "print ('---------------------------------------------------')\n",
    "print ('Fitness: ', cluster_33_1_full_fitness)\n",
    "print ('Precision: ', cluster_33_1_full_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conformance Checking - Cluster 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the conformance of the cluster 2 log with its own process model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConformanceChecking import ConformanceCheckingHandler\n",
    "\n",
    "# calculating the fitness & the precision of cluster 2 against own process model\n",
    "cluster_33_2_own_fitness, cluster_33_2_own_precision = ConformanceCheckingHandler.get_conformance(cluster_33_2_processs_model, cluster_33_2_event_log_actual_df, False)\n",
    "\n",
    "print ('Conformance of cluster 2 against own process model')\n",
    "print ('--------------------------------------------------')\n",
    "print ('Fitness: ', cluster_33_2_own_fitness)\n",
    "print ('Precision: ', cluster_33_2_own_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the conformance of the cluster 2 log with the process model of the full event log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConformanceChecking import ConformanceCheckingHandler\n",
    "\n",
    "# calculating the fitness & the precision of cluster 2 against full process model\n",
    "cluster_33_2_full_fitness, cluster_33_2_full_precision = ConformanceCheckingHandler.get_conformance(hospital_log_full_process_model, cluster_33_2_event_log_actual_df, False)\n",
    "\n",
    "print ('Conformance of cluster 2 against full process model')\n",
    "print ('---------------------------------------------------')\n",
    "print ('Fitness: ', cluster_33_2_full_fitness)\n",
    "print ('Precision: ', cluster_33_2_full_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conformance Checking - Cluster 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the conformance of the cluster 3 log with its own process model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConformanceChecking import ConformanceCheckingHandler\n",
    "\n",
    "# calculating the fitness & the precision of cluster 3 against own process model\n",
    "cluster_33_3_own_fitness, cluster_33_3_own_precision = ConformanceCheckingHandler.get_conformance(cluster_33_3_processs_model, cluster_33_3_event_log_actual_df, False)\n",
    "\n",
    "print ('Conformance of cluster 3 against own process model')\n",
    "print ('--------------------------------------------------')\n",
    "print ('Fitness: ', cluster_33_3_own_fitness)\n",
    "print ('Precision: ', cluster_33_3_own_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the conformance of the cluster 3 log with the process model of the full event log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConformanceChecking import ConformanceCheckingHandler\n",
    "\n",
    "# calculating the fitness & the precision of cluster 3 against full process model\n",
    "cluster_33_3_full_fitness, cluster_33_3_full_precision = ConformanceCheckingHandler.get_conformance(hospital_log_full_process_model, cluster_33_3_event_log_actual_df, False)\n",
    "\n",
    "print ('Conformance of cluster 3 against full process model')\n",
    "print ('---------------------------------------------------')\n",
    "print ('Fitness: ', cluster_33_3_full_fitness)\n",
    "print ('Precision: ', cluster_33_3_full_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conformance Checking - Cluster 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the conformance of the cluster 4 log with its own process model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConformanceChecking import ConformanceCheckingHandler\n",
    "\n",
    "# calculating the fitness & the precision of cluster 4 against own process model\n",
    "cluster_33_4_own_fitness, cluster_33_4_own_precision = ConformanceCheckingHandler.get_conformance(cluster_33_4_processs_model, cluster_33_4_event_log_actual_df, False)\n",
    "\n",
    "print ('Conformance of cluster 4 against own process model')\n",
    "print ('--------------------------------------------------')\n",
    "print ('Fitness: ', cluster_33_4_own_fitness)\n",
    "print ('Precision: ', cluster_33_4_own_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the conformance of the cluster 4 log with the process model of the full event log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConformanceChecking import ConformanceCheckingHandler\n",
    "\n",
    "# calculating the fitness & the precision of cluster 4 against full process model\n",
    "cluster_33_4_full_fitness, cluster_33_4_full_precision = ConformanceCheckingHandler.get_conformance(hospital_log_full_process_model, cluster_33_4_event_log_actual_df, False)\n",
    "\n",
    "print ('Conformance of cluster 4 against full process model')\n",
    "print ('---------------------------------------------------')\n",
    "print ('Fitness: ', cluster_33_4_full_fitness)\n",
    "print ('Precision: ', cluster_33_4_full_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conformance Checking - Cluster 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the conformance of the cluster 5 log with its own process model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConformanceChecking import ConformanceCheckingHandler\n",
    "\n",
    "# calculating the fitness & the precision of cluster 5 against own process model\n",
    "cluster_33_5_own_fitness, cluster_33_5_own_precision = ConformanceCheckingHandler.get_conformance(cluster_33_5_processs_model, cluster_33_5_event_log_actual_df, False)\n",
    "\n",
    "print ('Conformance of cluster 5 against own process model')\n",
    "print ('--------------------------------------------------')\n",
    "print ('Fitness: ', cluster_33_5_own_fitness)\n",
    "print ('Precision: ', cluster_33_5_own_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the conformance of the cluster 5 log with the process model of the full event log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConformanceChecking import ConformanceCheckingHandler\n",
    "\n",
    "# calculating the fitness & the precision of cluster 5 against full process model\n",
    "cluster_33_5_full_fitness, cluster_33_5_full_precision = ConformanceCheckingHandler.get_conformance(hospital_log_full_process_model, cluster_33_5_event_log_actual_df, False)\n",
    "\n",
    "print ('Conformance of cluster 5 against full process model')\n",
    "print ('---------------------------------------------------')\n",
    "print ('Fitness: ', cluster_33_5_full_fitness)\n",
    "print ('Precision: ', cluster_33_5_full_precision)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
