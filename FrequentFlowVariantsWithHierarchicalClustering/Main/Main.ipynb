{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the public Event Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Raw Event data in XES format as an Event Log ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pm4py\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.objects.conversion.log import converter as xes_converter\n",
    "\n",
    "hospital_log_all = xes_importer.apply('Data\\Hospital_log_all.xes')\n",
    "\n",
    "print(\"Number of traces present in the full event log:\", len(hospital_log_all))\n",
    "\n",
    "num_of_events = 0\n",
    "for trace in hospital_log_all:\n",
    "    num_of_events = num_of_events + len(trace)\n",
    "\n",
    "print(\"Number of events in full event log:\", num_of_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility code to automatically load newly compiled classes into Jupiter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "ip = get_ipython()\n",
    "ip.magic(\"reload_ext autoreload\")\n",
    "ip.magic(\"autoreload 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting a subset of the full event log based on traces ##\n",
    "\n",
    "We consider 33.3% of the traces in the full event log. This is for the ease of computation later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Preprocessing import DivideDatasets\n",
    "\n",
    "hospital_log_initial = DivideDatasets.get_subset(hospital_log_all, 3)\n",
    "\n",
    "print(\"Number of traces present in the event log subset:\", len(hospital_log_initial))\n",
    "\n",
    "num_of_events = 0\n",
    "for trace in hospital_log_initial:\n",
    "     num_of_events = num_of_events + len(trace)\n",
    "\n",
    "print(\"Number of events in the event log subset:\", num_of_events)\n",
    "\n",
    "pm4py.write_xes(hospital_log_initial, \"Data\\Processed\\Hospital_Log_Initial.xes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``When starting from middle with divided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pm4py\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.objects.conversion.log import converter as xes_converter\n",
    "\n",
    "hospital_log_initial = xes_importer.apply('Data\\Processed\\Hospital_Log_Initial.xes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating the event log content into English\n",
    "\n",
    "Mainly the activity name is in English. For a better clarity while analysing, we convert that to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hospital_log_initial_temp = hospital_log_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Preprocessing import TranslationManager\n",
    "\n",
    "hospital_log_translated = TranslationManager.translate(hospital_log_initial, 'en')\n",
    "\n",
    "pm4py.write_xes(hospital_log_translated, \"Data\\Processed\\Hospital_Log_Translated.xes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``When starting from middle (since Translation is costly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pm4py\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.objects.conversion.log import converter as xes_converter\n",
    "\n",
    "hospital_log_translated = xes_importer.apply('Data\\Processed\\Hospital_Log_Translated.xes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hospital_log_translated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove duplicates\n",
    "\n",
    "The logic here we are using is:\n",
    "  1. Extract traces & events into two lists from the original EventLog\n",
    "  2. Create a Pandas dataframe using above two lists as columns & remove duplicates from there\n",
    "  3. Rebuild the EventLog referring to original EventLog by only adding values persent in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pm4py.utils as pm4py_utils\n",
    "\n",
    "# Define neww lists for trace and event data\n",
    "all_traces = []\n",
    "all_events = []\n",
    "\n",
    "# Get trace and event data from the event log object\n",
    "\n",
    "###\n",
    "###\n",
    "#translation seems giving low hits\n",
    "#for trace in hospital_log_translated:\n",
    "###\n",
    "###\n",
    "\n",
    "for trace in hospital_log_initial:\n",
    "    # Get trace name for each trace\n",
    "    trace_name = trace.attributes['concept:name']\n",
    "\n",
    "    # Make the event name as unique as possible\n",
    "    for event in trace:\n",
    "        all_traces.append(trace_name)\n",
    "        # Extract event name\n",
    "        event_name = event['concept:name'] + ' - ' + event['Producer code'] + str(event['Specialism code']) + ' - ' + str(event['time:timestamp'])\n",
    "        if 'org:group' in event:\n",
    "            event_name = event_name + ' - ' + event['org:group'] \n",
    "        if 'Section' in event:\n",
    "            event_name = event_name + ' - ' + event['Section']\n",
    "            \n",
    "        all_events.append(event_name)\n",
    "\n",
    "# Raise error if the liists length are different\n",
    "if len(all_traces) != len(all_events):\n",
    "    raise ValueError(\"Lengths of all_traces and all_events do not match.\")\n",
    "\n",
    "# Creat a Pandas DataFrame\n",
    "hospital_log_initial_df = pd.DataFrame({'trace': all_traces, 'event': all_events})\n",
    "#print(\"All Events:\", len(hospital_log_translated_df))\n",
    "# Remove duplicates\n",
    "hospital_log_initial_df = hospital_log_initial_df.drop_duplicates(subset=['trace', 'event'])\n",
    "#print(\"Dup removed removed:\", len(hospital_log_translated_df))\n",
    "\n",
    "hospital_log_dup_removed = pm4py_utils.EventLog()\n",
    "\n",
    "# Initialize a dictionary to keep track of added events for each trace\n",
    "events_included = {}\n",
    "\n",
    "# Iterate over the original event log to reconstruct the dupliacate removed event log\n",
    "for trace in hospital_log_initial:\n",
    "    # Get trace name\n",
    "    trace_name = trace.attributes['concept:name']\n",
    "    \n",
    "    # Check if trace_name already exists in the DataFrame\n",
    "    if trace_name in hospital_log_initial_df['trace'].values:\n",
    "        # Create a new trace object\n",
    "        new_trace = pm4py_utils.Trace()\n",
    "        hospital_log_dup_removed.append(new_trace)\n",
    "        # Copy trace attributes\n",
    "        for key, value in trace.attributes.items():\n",
    "            new_trace.attributes[key] = value\n",
    "        # Initialize a set to keep track of added events for the current trace\n",
    "        events_included[trace_name] = set()\n",
    "        # Associate the trace name with all events within the trace\n",
    "        for event in trace:\n",
    "            event_name = event['concept:name'] + ' - ' + event['Producer code'] + str(event['Specialism code']) + ' - ' + str(event['time:timestamp'])\n",
    "            if 'org:group' in event:\n",
    "                event_name = event_name + ' - ' + event['org:group']\n",
    "            if 'Section' in event:\n",
    "                event_name = event_name + ' - ' + event['Section']\n",
    "            \n",
    "            # Check if the event name is in the cleaned DataFrame and not already added\n",
    "            if event_name in hospital_log_initial_df[hospital_log_initial_df['trace'] == trace_name]['event'].values \\\n",
    "                    and event_name not in events_included[trace_name]:\n",
    "                # Create a new event object\n",
    "                new_event = pm4py_utils.Event()\n",
    "                # Copy event attributes\n",
    "                for key, value in event.items():\n",
    "                    new_event[key] = value\n",
    "                # Add the event to the trace\n",
    "                new_trace.append(new_event)\n",
    "                # Add the event name to the set of added events for the current trace\n",
    "                events_included[trace_name].add(event_name)\n",
    "\n",
    "print(\"Number of traces present in the duplicate removed event log:\", len(hospital_log_dup_removed))\n",
    "\n",
    "num_of_events = 0\n",
    "for trace in hospital_log_dup_removed:\n",
    "     num_of_events = num_of_events + len(trace)\n",
    "\n",
    "print(\"Number of events in the duplicate removed event:\", num_of_events)\n",
    "\n",
    "# write into file\n",
    "pm4py.write_xes(hospital_log_dup_removed, \"Data\\Processed\\Hospital_Log_Dup_Removed.xes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``When starting from middle (dup removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pm4py\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.objects.conversion.log import converter as xes_converter\n",
    "\n",
    "hospital_log_dup_removed = xes_importer.apply('Data\\Processed\\Hospital_Log_Dup_Removed.xes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove traces with no events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def count_traces_without_events(Logg):\n",
    "    \n",
    "    # Count traces without events\n",
    "    count = 0\n",
    "    all_traces=0\n",
    "    for trace in Logg:\n",
    "        # Check if trace has any events\n",
    "        all_traces += 1\n",
    "        present = 0\n",
    "        for event in trace:\n",
    "            present += 1\n",
    "        if present==0:\n",
    "            count += 1\n",
    "        if all_traces >379:\n",
    "            print(all_traces, \": \", present)\n",
    "\n",
    "    return count, all_traces\n",
    "\n",
    "traces_without_events, all_tracesss = count_traces_without_events(hospital_log_translated_emp)\n",
    "print(\"Number of traces:\", all_tracesss)\n",
    "print(\"Number of traces without any events:\", traces_without_events)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequent Pattern Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding most frequest patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using FP-Growth algorithm\n",
    "\n",
    "We are using the fp growth algorithm here to find frequent flow variants.\n",
    "In creating transactions to be fed into into FP Growth, we use concept:name as the key of each event\n",
    "Also sending the transaction list in chunks to FP Growth algo to process to reduce the complexity in processing,\n",
    "and later merging the results by removing duplicates, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from FrequentPatternMining import FPGrowthHandler, TransactionManager\n",
    "\n",
    "\n",
    "# GEt the list of transactions based on events in each trace\n",
    "transactions_list = TransactionManager.create_transactions(hospital_log_dup_removed)\n",
    "\n",
    "# Split transactions list into chunks \n",
    "# for the computational ease\n",
    "chunk_size = 5\n",
    "num_of_chunks = len(transactions_list) // chunk_size +(len(transactions_list) % chunk_size > 0)\n",
    "frequent_variants_all = pd.DataFrame( columns=['support', 'itemsets'])\n",
    "\n",
    "for i in range(num_of_chunks):\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = min((i + 1) * chunk_size, len(transactions_list))\n",
    "    transactions_chunk = transactions_list[start_idx:end_idx]\n",
    "    \n",
    "    # Mine frequent varints for the current chunk using FP-Growth algorithm\n",
    "    print(f\"Processing chunk {i+1} of {num_of_chunks}...\")\n",
    "    frequent_variants_chunk = FPGrowthHandler.mine_frequent_variants(transactions_chunk, min_support=0.1)\n",
    "    \n",
    "    # Merge frequent variants with previous chunks\n",
    "    frequent_variants_all = pd.concat([frequent_variants_all, frequent_itemsets_chunk])\n",
    "    print(\"Mining finshed for chunk {i+1} of {num_of_chunks}...\")\n",
    "\n",
    "# Remove duplicates and sort the combined frequent variants\n",
    "frequent_variants_all = frequent_variants_all.groupby('itemsets').agg({'support': 'sum'}).reset_index()\n",
    "frequent_variants_all = frequent_variants_all.sort_values(by='support', ascending=False)\n",
    "\n",
    "print(\"Full Frequent Variants List: \")\n",
    "frequent_variants_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from FrequentPatternMining import AprioriHandler, TransactionManager\n",
    "\n",
    "\n",
    "# GEt the list of transactions based on events in each trace\n",
    "transactions_list = TransactionManager.create_transactions(hospital_log_dup_removed)\n",
    "\n",
    "# Split transactions list into chunks \n",
    "# for the computational ease\n",
    "chunk_size = 5\n",
    "num_of_chunks = len(transactions_list) // chunk_size +(len(transactions_list) % chunk_size > 0)\n",
    "frequent_variants_all = pd.DataFrame( columns=['support', 'itemsets'])\n",
    "\n",
    "for i in range(num_of_chunks):\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = min((i + 1) * chunk_size, len(transactions_list))\n",
    "    transactions_chunk = transactions_list[start_idx:end_idx]\n",
    "    \n",
    "    # Mine frequent variantsa for the current chunk using Apriori algorithm\n",
    "    print(f\"Processing chunk {i+1} of {num_of_chunks}...\")\n",
    "    frequent_variants_chunk = AprioriHandler.mine_frequent_variants(transactions_chunk, min_support=0.1)\n",
    "    \n",
    "    # Merge frequent varints with previous chunks\n",
    "    frequent_variants_all = pd.concat([frequent_variants_all, frequent_itemsets_chunk])\n",
    "    print(\"Mining finshed for chunk {i+1} of {num_of_chunks}...\")\n",
    "\n",
    "# Remove duplicates and sort the combined frequent variants\n",
    "frequent_variants_all = frequent_variants_all.groupby('itemsets').agg({'support': 'sum'}).reset_index()\n",
    "frequent_variants_all = frequent_variants_all.sort_values(by='support', ascending=False)\n",
    "\n",
    "print(\"Full Frequent Variants List: \")\n",
    "frequent_variants_all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
